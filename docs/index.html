<!DOCTYPE html><html class="default" lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="IE=edge"/><title>@wllama/wllama</title><meta name="description" content="Documentation for @wllama/wllama"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="assets/style.css"/><link rel="stylesheet" href="assets/highlight.css"/><script defer src="assets/main.js"></script><script async src="assets/icons.js" id="tsd-icons-script"></script><script async src="assets/search.js" id="tsd-search-script"></script><script async src="assets/navigation.js" id="tsd-nav-script"></script></head><body><script>document.documentElement.dataset.theme = localStorage.getItem("tsd-theme") || "os";document.body.style.display="none";setTimeout(() => app?app.showPage():document.body.style.removeProperty("display"),500)</script><header class="tsd-page-toolbar"><div class="tsd-toolbar-contents container"><div class="table-cell" id="tsd-search" data-base="."><div class="field"><label for="tsd-search-field" class="tsd-widget tsd-toolbar-icon search no-caption"><svg width="16" height="16" viewBox="0 0 16 16" fill="none"><use href="assets/icons.svg#icon-search"></use></svg></label><input type="text" id="tsd-search-field" aria-label="Search"/></div><div class="field"><div id="tsd-toolbar-links"></div></div><ul class="results"><li class="state loading">Preparing search index...</li><li class="state failure">The search index is not available</li></ul><a href="index.html" class="title">@wllama/wllama</a></div><div class="table-cell" id="tsd-widgets"><a href="#" class="tsd-widget tsd-toolbar-icon menu no-caption" data-toggle="menu" aria-label="Menu"><svg width="16" height="16" viewBox="0 0 16 16" fill="none"><use href="assets/icons.svg#icon-menu"></use></svg></a></div></div></header><div class="container container-main"><div class="col-content"><div class="tsd-page-title"><h2>@wllama/wllama</h2></div><div class="tsd-panel tsd-typography"><a id="md:wllama---wasm-binding-for-llamacpp" class="tsd-anchor"></a><h1><a href="#md:wllama---wasm-binding-for-llamacpp">wllama - Wasm binding for llama.cpp</a></h1><p><img src="./README_banner.png" alt=""></p>
<p>Another WebAssembly binding for <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>. Inspired by <a href="https://github.com/tangledgroup/llama-cpp-wasm">tangledgroup/llama-cpp-wasm</a>, but unlike it, <strong>Wllama</strong> aims to supports <strong>low-level API</strong> like (de)tokenization, embeddings,...</p>
<a id="md:recent-changes" class="tsd-anchor"></a><h2><a href="#md:recent-changes">Recent changes</a></h2><ul>
<li>Version 1.7.0<ul>
<li>When downloading, <code>n_downloads_parallel</code> is changed to <code>parallelDownloads</code></li>
<li>Added support for <code>progressCallback</code> when downloading. See <a href="./examples/advanced/index.html">advanced example</a></li>
</ul>
</li>
<li>Version 1.5.0<ul>
<li>Support split model using <a href="https://github.com/ggerganov/llama.cpp/tree/master/examples/gguf-split">gguf-split tool</a></li>
</ul>
</li>
<li>Version 1.4.0<ul>
<li>Add <code>single-thread/wllama.js</code> and <code>multi-thread/wllama.js</code> to the list of <code>CONFIG_PATHS</code></li>
<li><code>createEmbedding</code> is now adding BOS and EOS token by default</li>
</ul>
</li>
</ul>
<a id="md:features" class="tsd-anchor"></a><h2><a href="#md:features">Features</a></h2><ul>
<li>Typescript support</li>
<li>Can run inference directly on browser (using <a href="https://emscripten.org/docs/porting/simd.html">WebAssembly SIMD</a>), no backend or GPU is needed!</li>
<li>No runtime dependency (see <a href="./package.json">package.json</a>)</li>
<li>High-level API: completions, embeddings</li>
<li>Low-level API: (de)tokenize, KV cache control, sampling control,...</li>
<li>Ability to split the model into smaller files and load them in parallel (same as <code>split</code> and <code>cat</code>)</li>
<li>Auto switch between single-thread and multi-thread build based on browser support</li>
<li>Inference is done inside a worker, does not block UI render</li>
<li>Pre-built npm package <a href="https://www.npmjs.com/package/@wllama/wllama">@wllama/wllama</a></li>
</ul>
<p>Limitations:</p>
<ul>
<li>To enable multi-thread, you must add <code>Cross-Origin-Embedder-Policy</code> and <code>Cross-Origin-Opener-Policy</code> headers. See <a href="https://github.com/ffmpegwasm/ffmpeg.wasm/issues/106#issuecomment-913450724">this discussion</a> for more details.</li>
<li>No WebGL support, but maybe possible in the future</li>
<li>Max file size is 2GB, due to <a href="https://stackoverflow.com/questions/17823225/do-arraybuffers-have-a-maximum-length">size restriction of ArrayBuffer</a>. If your model is bigger than 2GB, please follow the <strong>Split model</strong> section below.</li>
</ul>
<a id="md:demo-and-documentations" class="tsd-anchor"></a><h2><a href="#md:demo-and-documentations">Demo and documentations</a></h2><p><strong>Documentation:</strong> <a href="https://ngxson.github.io/wllama/docs/">https://ngxson.github.io/wllama/docs/</a></p>
<p>Demo:</p>
<ul>
<li>Basic usages with completions and embeddings: <a href="https://ngxson.github.io/wllama/examples/basic/">https://ngxson.github.io/wllama/examples/basic/</a></li>
<li>Advanced example using low-level API: <a href="https://ngxson.github.io/wllama/examples/advanced/">https://ngxson.github.io/wllama/examples/advanced/</a></li>
<li>Embedding and cosine distance: <a href="https://ngxson.github.io/wllama/examples/embeddings/">https://ngxson.github.io/wllama/examples/embeddings/</a></li>
</ul>
<a id="md:how-to-use" class="tsd-anchor"></a><h2><a href="#md:how-to-use">How to use</a></h2><a id="md:use-wllama-inside-react-typescript-project" class="tsd-anchor"></a><h3><a href="#md:use-wllama-inside-react-typescript-project">Use Wllama inside React Typescript project</a></h3><p>Install it:</p>
<pre><code class="language-bash"><span class="hl-0">npm</span><span class="hl-1"> </span><span class="hl-2">i</span><span class="hl-1"> </span><span class="hl-2">@wllama/wllama</span>
</code><button>Copy</button></pre>
<p>For complete code, see <a href="./examples/reactjs">examples/reactjs</a></p>
<p>NOTE: this example only covers completions usage. For embeddings, please see <a href="./examples/embeddings/index.html">examples/embeddings/index.html</a></p>
<a id="md:prepare-your-model" class="tsd-anchor"></a><h3><a href="#md:prepare-your-model">Prepare your model</a></h3><ul>
<li>It is recommended to split the model into <strong>chunks of maximum 512MB</strong>. This will result in slightly faster download speed (because multiple splits can be downloaded in parallel), and also prevent some out-of-memory issues.<br>See the &quot;Split model&quot; section below for more details.</li>
<li>It is recommended to use quantized Q4, Q5 or Q6 for balance among performance, file size and quality. Using IQ (with imatrix) is <strong>not</strong> recommended, may result in slow inference and low quality.</li>
</ul>
<a id="md:simple-usage-with-es6-module" class="tsd-anchor"></a><h3><a href="#md:simple-usage-with-es6-module">Simple usage with ES6 module</a></h3><p>For complete code, see <a href="./examples/basic/index.html">examples/basic/index.html</a></p>
<pre><code class="language-javascript"><span class="hl-3">import</span><span class="hl-1"> { </span><span class="hl-4">Wllama</span><span class="hl-1"> } </span><span class="hl-3">from</span><span class="hl-1"> </span><span class="hl-2">&#39;./esm/index.js&#39;</span><span class="hl-1">;</span><br/><br/><span class="hl-1">(</span><span class="hl-5">async</span><span class="hl-1"> () </span><span class="hl-5">=&gt;</span><span class="hl-1"> {</span><br/><span class="hl-1">  </span><span class="hl-5">const</span><span class="hl-1"> </span><span class="hl-6">CONFIG_PATHS</span><span class="hl-1"> = {</span><br/><span class="hl-1">    </span><span class="hl-2">&#39;single-thread/wllama.js&#39;</span><span class="hl-4">       :</span><span class="hl-1"> </span><span class="hl-2">&#39;./esm/single-thread/wllama.js&#39;</span><span class="hl-1">,</span><br/><span class="hl-1">    </span><span class="hl-2">&#39;single-thread/wllama.wasm&#39;</span><span class="hl-4">     :</span><span class="hl-1"> </span><span class="hl-2">&#39;./esm/single-thread/wllama.wasm&#39;</span><span class="hl-1">,</span><br/><span class="hl-1">    </span><span class="hl-2">&#39;multi-thread/wllama.js&#39;</span><span class="hl-4">        :</span><span class="hl-1"> </span><span class="hl-2">&#39;./esm/multi-thread/wllama.js&#39;</span><span class="hl-1">,</span><br/><span class="hl-1">    </span><span class="hl-2">&#39;multi-thread/wllama.wasm&#39;</span><span class="hl-4">      :</span><span class="hl-1"> </span><span class="hl-2">&#39;./esm/multi-thread/wllama.wasm&#39;</span><span class="hl-1">,</span><br/><span class="hl-1">    </span><span class="hl-2">&#39;multi-thread/wllama.worker.mjs&#39;</span><span class="hl-4">:</span><span class="hl-1"> </span><span class="hl-2">&#39;./esm/multi-thread/wllama.worker.mjs&#39;</span><span class="hl-1">,</span><br/><span class="hl-1">  };</span><br/><span class="hl-1">  </span><span class="hl-7">// Automatically switch between single-thread and multi-thread version based on browser support</span><br/><span class="hl-1">  </span><span class="hl-7">// If you want to enforce single-thread, add { &quot;n_threads&quot;: 1 } to LoadModelConfig</span><br/><span class="hl-1">  </span><span class="hl-5">const</span><span class="hl-1"> </span><span class="hl-6">wllama</span><span class="hl-1"> = </span><span class="hl-5">new</span><span class="hl-1"> </span><span class="hl-0">Wllama</span><span class="hl-1">(</span><span class="hl-6">CONFIG_PATHS</span><span class="hl-1">);</span><br/><span class="hl-1">  </span><span class="hl-7">// Define a function for tracking the model download progress</span><br/><span class="hl-1">  </span><span class="hl-5">const</span><span class="hl-1"> </span><span class="hl-0">progressCallback</span><span class="hl-1"> =  ({ </span><span class="hl-4">loaded</span><span class="hl-1">, </span><span class="hl-4">total</span><span class="hl-1"> }) </span><span class="hl-5">=&gt;</span><span class="hl-1"> {</span><br/><span class="hl-1">    </span><span class="hl-7">// Calculate the progress as a percentage</span><br/><span class="hl-1">    </span><span class="hl-5">const</span><span class="hl-1"> </span><span class="hl-6">progressPercentage</span><span class="hl-1"> = </span><span class="hl-4">Math</span><span class="hl-1">.</span><span class="hl-0">round</span><span class="hl-1">((</span><span class="hl-4">loaded</span><span class="hl-1"> / </span><span class="hl-4">total</span><span class="hl-1">) * </span><span class="hl-8">100</span><span class="hl-1">);</span><br/><span class="hl-1">    </span><span class="hl-7">// Log the progress in a user-friendly format</span><br/><span class="hl-1">    </span><span class="hl-4">console</span><span class="hl-1">.</span><span class="hl-0">log</span><span class="hl-1">(</span><span class="hl-2">`Downloading... </span><span class="hl-5">${</span><span class="hl-4">progressPercentage</span><span class="hl-5">}</span><span class="hl-2">%`</span><span class="hl-1">);</span><br/><span class="hl-1">  };</span><br/><span class="hl-1">  </span><span class="hl-3">await</span><span class="hl-1"> </span><span class="hl-4">wllama</span><span class="hl-1">.</span><span class="hl-0">loadModelFromUrl</span><span class="hl-1">(</span><br/><span class="hl-1">    </span><span class="hl-2">&quot;https://huggingface.co/ggml-org/models/resolve/main/tinyllamas/stories260K.gguf&quot;</span><span class="hl-1">,</span><br/><span class="hl-1">    {</span><br/><span class="hl-1">      </span><span class="hl-4">progressCallback</span><span class="hl-1">,</span><br/><span class="hl-1">    }</span><br/><span class="hl-1">  );</span><br/><span class="hl-1">  </span><span class="hl-5">const</span><span class="hl-1"> </span><span class="hl-6">outputText</span><span class="hl-1"> = </span><span class="hl-3">await</span><span class="hl-1"> </span><span class="hl-4">wllama</span><span class="hl-1">.</span><span class="hl-0">createCompletion</span><span class="hl-1">(</span><span class="hl-4">elemInput</span><span class="hl-1">.</span><span class="hl-4">value</span><span class="hl-1">, {</span><br/><span class="hl-1">    </span><span class="hl-4">nPredict:</span><span class="hl-1"> </span><span class="hl-8">50</span><span class="hl-1">,</span><br/><span class="hl-1">    </span><span class="hl-4">sampling:</span><span class="hl-1"> {</span><br/><span class="hl-1">      </span><span class="hl-4">temp:</span><span class="hl-1"> </span><span class="hl-8">0.5</span><span class="hl-1">,</span><br/><span class="hl-1">      </span><span class="hl-4">top_k:</span><span class="hl-1"> </span><span class="hl-8">40</span><span class="hl-1">,</span><br/><span class="hl-1">      </span><span class="hl-4">top_p:</span><span class="hl-1"> </span><span class="hl-8">0.9</span><span class="hl-1">,</span><br/><span class="hl-1">    },</span><br/><span class="hl-1">  });</span><br/><span class="hl-1">  </span><span class="hl-4">console</span><span class="hl-1">.</span><span class="hl-0">log</span><span class="hl-1">(</span><span class="hl-4">outputText</span><span class="hl-1">);</span><br/><span class="hl-1">})();</span>
</code><button>Copy</button></pre>
<a id="md:split-model" class="tsd-anchor"></a><h3><a href="#md:split-model">Split model</a></h3><p>Cases where we want to split the model:</p>
<ul>
<li>Due to <a href="https://stackoverflow.com/questions/17823225/do-arraybuffers-have-a-maximum-length">size restriction of ArrayBuffer</a>, the size limitation of a file is 2GB. If your model is bigger than 2GB, you can split the model into small files.</li>
<li>Even with a small model, splitting into chunks allows the browser to download multiple chunks in parallel, thus making the download process a bit faster.</li>
</ul>
<p>We use <code>gguf-split</code> to split a big gguf file into smaller files. You can download the pre-built binary via <a href="https://github.com/ggerganov/llama.cpp/releases">llama.cpp release page</a>:</p>
<pre><code class="language-bash"><span class="hl-7"># Split the model into chunks of 512 Megabytes</span><br/><span class="hl-0">./gguf-split</span><span class="hl-1"> </span><span class="hl-5">--split-max-size</span><span class="hl-1"> </span><span class="hl-8">512</span><span class="hl-2">M</span><span class="hl-1"> </span><span class="hl-2">./my_model.gguf</span><span class="hl-1"> </span><span class="hl-2">./my_model</span>
</code><button>Copy</button></pre>
<p>This will output files ending with <code>-00001-of-00003.gguf</code>, <code>-00002-of-00003.gguf</code>,...</p>
<p>You can then give a list of uploaded files to <code>loadModelFromUrl</code>:</p>
<pre><code class="language-js"><span class="hl-3">await</span><span class="hl-1"> </span><span class="hl-4">wllama</span><span class="hl-1">.</span><span class="hl-0">loadModelFromUrl</span><span class="hl-1">(</span><br/><span class="hl-1">  [</span><br/><span class="hl-1">    </span><span class="hl-2">&#39;https://huggingface.co/ngxson/tinyllama_split_test/resolve/main/stories15M-q8_0-00001-of-00003.gguf&#39;</span><span class="hl-1">,</span><br/><span class="hl-1">    </span><span class="hl-2">&#39;https://huggingface.co/ngxson/tinyllama_split_test/resolve/main/stories15M-q8_0-00002-of-00003.gguf&#39;</span><span class="hl-1">,</span><br/><span class="hl-1">    </span><span class="hl-2">&#39;https://huggingface.co/ngxson/tinyllama_split_test/resolve/main/stories15M-q8_0-00003-of-00003.gguf&#39;</span><span class="hl-1">,</span><br/><span class="hl-1">  ],</span><br/><span class="hl-1">  {</span><br/><span class="hl-1">    </span><span class="hl-4">parallelDownloads:</span><span class="hl-1"> </span><span class="hl-8">5</span><span class="hl-1">, </span><span class="hl-7">// optional: maximum files to download in parallel (default: 3)</span><br/><span class="hl-1">  },</span><br/><span class="hl-1">);</span>
</code><button>Copy</button></pre>
<a id="md:how-to-compile-the-binary-yourself" class="tsd-anchor"></a><h2><a href="#md:how-to-compile-the-binary-yourself">How to compile the binary yourself</a></h2><p>This repository already come with pre-built binary from llama.cpp source code. However, in some cases you may want to compile it yourself:</p>
<ul>
<li>You don&#39;t trust the pre-built one.</li>
<li>You want to try out latest - bleeding-edge changes from upstream llama.cpp source code.</li>
</ul>
<p>You can use the commands below to compile it yourself:</p>
<pre><code class="language-shell"><span class="hl-7"># /!\ IMPORTANT: Require having docker compose installed</span><br/><br/><span class="hl-7"># Clone the repository with submodule</span><br/><span class="hl-0">git</span><span class="hl-1"> </span><span class="hl-2">clone</span><span class="hl-1"> </span><span class="hl-5">--recurse-submodules</span><span class="hl-1"> </span><span class="hl-2">https://github.com/ngxson/wllama.git</span><br/><span class="hl-0">cd</span><span class="hl-1"> </span><span class="hl-2">wllama</span><br/><br/><span class="hl-7"># Optionally, you can run this command to update llama.cpp to latest upstream version (bleeding-edge, use with your own risk!)</span><br/><span class="hl-7"># git submodule update --remote --merge</span><br/><br/><span class="hl-7"># Firstly, build llama.cpp into wasm</span><br/><span class="hl-0">npm</span><span class="hl-1"> </span><span class="hl-2">run</span><span class="hl-1"> </span><span class="hl-2">build:wasm</span><br/><span class="hl-7"># Then, build ES module</span><br/><span class="hl-0">npm</span><span class="hl-1"> </span><span class="hl-2">run</span><span class="hl-1"> </span><span class="hl-2">build</span>
</code><button>Copy</button></pre>
<a id="md:todo" class="tsd-anchor"></a><h2><a href="#md:todo">TODO</a></h2><p>Short term:</p>
<ul>
<li>Add a more pratical embedding example (using a better model)</li>
<li>Maybe doing a full RAG-in-browser example using tinyllama?</li>
</ul>
<p>Long term:</p>
<ul>
<li>Support GPU inference via WebGL</li>
<li>Support multi-sequences: knowing the resource limitation when using WASM, I don&#39;t think having multi-sequences is a good idea</li>
<li>Multi-modal: Waiting for refactoring LLaVA implementation from llama.cpp</li>
</ul>
</div></div><div class="col-sidebar"><div class="page-menu"><div class="tsd-navigation settings"><details class="tsd-index-accordion"><summary class="tsd-accordion-summary"><h3><svg width="20" height="20" viewBox="0 0 24 24" fill="none"><use href="assets/icons.svg#icon-chevronDown"></use></svg>Settings</h3></summary><div class="tsd-accordion-details"><div class="tsd-filter-visibility"><h4 class="uppercase">Member Visibility</h4><form><ul id="tsd-filter-options"><li class="tsd-filter-item"><label class="tsd-filter-input"><input type="checkbox" id="tsd-filter-protected" name="protected"/><svg width="32" height="32" viewBox="0 0 32 32" aria-hidden="true"><rect class="tsd-checkbox-background" width="30" height="30" x="1" y="1" rx="6" fill="none"></rect><path class="tsd-checkbox-checkmark" d="M8.35422 16.8214L13.2143 21.75L24.6458 10.25" stroke="none" stroke-width="3.5" stroke-linejoin="round" fill="none"></path></svg><span>Protected</span></label></li><li class="tsd-filter-item"><label class="tsd-filter-input"><input type="checkbox" id="tsd-filter-private" name="private"/><svg width="32" height="32" viewBox="0 0 32 32" aria-hidden="true"><rect class="tsd-checkbox-background" width="30" height="30" x="1" y="1" rx="6" fill="none"></rect><path class="tsd-checkbox-checkmark" d="M8.35422 16.8214L13.2143 21.75L24.6458 10.25" stroke="none" stroke-width="3.5" stroke-linejoin="round" fill="none"></path></svg><span>Private</span></label></li><li class="tsd-filter-item"><label class="tsd-filter-input"><input type="checkbox" id="tsd-filter-inherited" name="inherited" checked/><svg width="32" height="32" viewBox="0 0 32 32" aria-hidden="true"><rect class="tsd-checkbox-background" width="30" height="30" x="1" y="1" rx="6" fill="none"></rect><path class="tsd-checkbox-checkmark" d="M8.35422 16.8214L13.2143 21.75L24.6458 10.25" stroke="none" stroke-width="3.5" stroke-linejoin="round" fill="none"></path></svg><span>Inherited</span></label></li><li class="tsd-filter-item"><label class="tsd-filter-input"><input type="checkbox" id="tsd-filter-external" name="external"/><svg width="32" height="32" viewBox="0 0 32 32" aria-hidden="true"><rect class="tsd-checkbox-background" width="30" height="30" x="1" y="1" rx="6" fill="none"></rect><path class="tsd-checkbox-checkmark" d="M8.35422 16.8214L13.2143 21.75L24.6458 10.25" stroke="none" stroke-width="3.5" stroke-linejoin="round" fill="none"></path></svg><span>External</span></label></li></ul></form></div><div class="tsd-theme-toggle"><h4 class="uppercase">Theme</h4><select id="tsd-theme"><option value="os">OS</option><option value="light">Light</option><option value="dark">Dark</option></select></div></div></details></div><details open class="tsd-index-accordion tsd-page-navigation"><summary class="tsd-accordion-summary"><h3><svg width="20" height="20" viewBox="0 0 24 24" fill="none"><use href="assets/icons.svg#icon-chevronDown"></use></svg>On This Page</h3></summary><div class="tsd-accordion-details"><a href="#md:wllama---wasm-binding-for-llamacpp"><span>wllama -<wbr/> <wbr/>Wasm binding for llama.cpp</span></a><ul><li><a href="#md:recent-changes"><span>Recent changes</span></a></li><li><a href="#md:features"><span>Features</span></a></li><li><a href="#md:demo-and-documentations"><span>Demo and documentations</span></a></li><li><a href="#md:how-to-use"><span>How to use</span></a></li><li><ul><li><a href="#md:use-wllama-inside-react-typescript-project"><span>Use <wbr/>Wllama inside <wbr/>React <wbr/>Typescript project</span></a></li><li><a href="#md:prepare-your-model"><span>Prepare your model</span></a></li><li><a href="#md:simple-usage-with-es6-module"><span>Simple usage with ES6 module</span></a></li><li><a href="#md:split-model"><span>Split model</span></a></li></ul></li><li><a href="#md:how-to-compile-the-binary-yourself"><span>How to compile the binary yourself</span></a></li><li><a href="#md:todo"><span>TODO</span></a></li></ul></div></details></div><div class="site-menu"><nav class="tsd-navigation"><a href="modules.html" class="current"><svg class="tsd-kind-icon" viewBox="0 0 24 24"><use href="assets/icons.svg#icon-1"></use></svg><span>@wllama/wllama</span></a><ul class="tsd-small-nested-navigation" id="tsd-nav-container" data-base="."><li>Loading...</li></ul></nav></div></div></div><div class="tsd-generator"><p>Generated using <a href="https://typedoc.org/" target="_blank">TypeDoc</a></p></div><div class="overlay"></div></body></html>